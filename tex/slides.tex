\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{pst-plot}
\usepackage{amssymb}
\usepackage{pifont}

% Code snippets
\usepackage{minted}
\definecolor{rulecolor}{rgb}{0.80,0.80,0.80}
\definecolor{bgcolor}{rgb}{1.0,1.0,1.0}
\newminted{python}{bgcolor=bgcolor}

% Checked marks
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% Colors
\newrgbcolor{mygreen}{.00 .5 .00}
\newcommand{\X}[1]{\textcolor{blue}{#1}}
\newcommand{\y}[1]{\textcolor{red}{#1}}
\newcommand{\model}[1]{\textcolor{mygreen}{#1}}
\newcommand{\loss}[1]{\textcolor{lightblue}{#1}}

% Beamer layout
\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue}
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty
\setbeamertemplate{sections/subsections in toc}[circle]
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{itemize items}[circle]
\setbeamertemplate{itemize subitem}[square]

% Argmax
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% Front slide
\title{{\bf Pitfalls of evaluating a classifier's performance in high energy physics applications}}
\author{
Gilles Louppe, NYU (\href{https://twitter.com/glouppe}{@glouppe}) \\
Tim Head, EPFL (\href{https://twitter.com/betatim}{@betatim}) \\
}
\date{December 11, 2015\\
ALEPH workshop, NIPS}

\begin{document}

\begin{frame}[plain]
\titlepage
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{Disclaimer}

The following applies {\color{red} only} for the learning protocol of the {\it
Flavours of Physics} Kaggle challenge \citep{blake2014}.

\vspace{1cm}

See \citep{louppe2014} for the notebook explanations.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{Flavours of Physics: Finding $\tau \mapsto \mu\mu\mu$ challenge}

Given a learning set ${\cal L}$ of

\vspace{0.5cm}

\begin{itemize}
\item simulated signal events $(\mathbf{x}, s)$
\item real data background events $(\mathbf{x}, b)$,
\end{itemize}

\vspace{0.5cm}

build a classifier $\varphi : {\cal X} \mapsto \{s, b\}$ for distinguishing
$\tau \mapsto \mu\mu\mu$ signal events from background events.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{Control channel test}

The simulation is not perfect: {\color{red} simulated and real data events can
often be distinguished}.

\vspace{1cm}

To avoid exploiting simulation versus real data artefacts to classify signal
from background events, we {\color{blue}evaluate whether $\varphi$ behaves
differently on simulated signal and real data signal from a control channel
${\cal C}$}.

\vspace{1cm}

Control channel test: Requires the Kolmogorov-Smirnov test statistic between $\{
\varphi(\mathbf{x}) | \mathbf{x} \in {\cal C}^\text{sim} \}$ and $\{
\varphi(\mathbf{x}) | \mathbf{x} \in {\cal C}^\text{data} \}$ to be strictly
smaller than some pre-defined threshold $t$.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{Loophole}

If control data can be distinguished from training data,
then there exist classifiers $\varphi$ exploiting simulation artefacts
 to classify signal from background events, for which {\color{red}the
control channel test succeeds}.

\vspace{0.5cm}

Therefore,

\begin{itemize}

\item The true performance of $\varphi$ on real data may be significantly
different (typically lower) than estimated on simulated signal events versus
real data background events.

\item Passing the KS test should not be interpreted as $\varphi$
not exploiting simulation versus real data artefacts.

\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{Toy example}

Let us consider an artificial classification problem between signal and
background events, along with some close control channel data ${\cal
C}^\text{sim}$ and ${\cal C}^\text{data}$.

\vspace{0.5cm}

Let us assume an input space defined on three input variables $X_1$, $X_2$,
$X_3$ as follows.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/x1.png}
\end{figure}

$X_1$ is {\color{red} irrelevant} for real data signal versus real data
background, but relevant for simulated versus real data events.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/x2.png}
\end{figure}

$X_2$ is {\color{blue} relevant} for  background and non-background events.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/x3.png}
\end{figure}

$X_3$ is relevant for training versus control events, but has otherwise no
discriminative power between signal and background events.

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Random exploration}

{\scriptsize
\begin{pythoncode}
def find_best_tree(X_train, y_train, X_test, y_test,
                   X_data, y_data, X_control_sim, X_control_data):
    best_auc_test, best_auc_data = 0, 0
    best_ks = 0
    best_tree = None

    for seed in range(2000):
        clf = ExtraTreesClassifier(n_estimators=1, max_features=1,
                                   max_leaf_nodes=5, random_state=seed)
        clf.fit(X_train, y_train)
        auc_test = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
        auc_data = roc_auc_score(y_data, clf.predict_proba(X_data)[:, 1])
        ks = ks_statistic(clf.predict_proba(X_control_sim)[:, 1],
                          clf.predict_proba(X_control_data)[:, 1])

        if auc_test > best_auc_test and ks < 0.09:
            best_auc_test = auc_test
            best_auc_data = auc_data
            best_ks = ks
            best_tree = clf

    return best_auc_test, best_auc_data, best_ks, best_tree
\end{pythoncode}
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Random exploration}

{\scriptsize
\begin{pythoncode}
auc_test, auc_data, ks, tree = find_best_tree(...)
>>> auc_test = 0.9863  # Estimated AUC (simulated signal vs. data background)
>>> ks = 0.0578        # KS statistic < 0.09
\end{pythoncode}
}

\vspace{0.5cm}

What just happened? By chance, we have found a classifier that
\begin{itemize}
\item has seemingly good test performance;
\item passes the control channel test that we have defined.
\end{itemize}
{\color{blue} This classifier appears to be exactly the one we were seeking}.

\vspace{0.5cm}

{\color{red}Wrong}. The expected ROC AUC on real data signal and real
data background is significantly lower than our first estimate, suggesting that
there is still something wrong.

\vspace{0.5cm}

{\scriptsize
\begin{pythoncode}
>>> auc_data = 0.9097  # True AUC (data signal vs. data background)
\end{pythoncode}
}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/tree.png}
\end{figure}

$\varphi$ exploits $X_1$, i.e. simulation versus real data
artefacts to indirectly classify signal from background events, {\color{red}while still passing the
control channel test} because of its use of $X_3$!

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{Winning the challenge}

\begin{enumerate}
\item Learn to distinguish between training and control data,
\item Build a classifier on training data, with all the freedom to exploit simulation artefacts,
\item Assign random predictions to samples predicted as control data, otherwise predict using the classifier found in the previous step.
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/hole.png}\\
{\it The reconstructed mass allows to distinguish {\color{red} signal} from {\color{blue} background} and training from {\color{mygreen} control}!}
\end{figure}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}
\frametitle{A machine learning response}

As simulated training data increases (i.e., as $N \to \infty$),

$$\frac{1}{N} \sum_{\mathbf{x}_i} L(\varphi(\mathbf{x}_i)) \to \int L(\varphi(\mathbf{x})) {\color{red} p_\text{sim}(\mathbf{x})} d\mathbf{x}.$$

We want to be good on real data, i.e., minimize
$$\int L(\varphi(\mathbf{x})) {\color{blue} p_\text{data}(\mathbf{x})} d\mathbf{x}.$$

Solution: {\it importance weighting}.

$$\varphi^* = \argmin_\varphi \frac{1}{N} \sum_{\mathbf{x}_i} \frac{\color{blue} p_\text{data}(\mathbf{x}_i)}{\color{red} p_\text{sim}(\mathbf{x}_i)}  L(\varphi(\mathbf{x}_i))$$

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Density ratio estimation}
But for signal events, we don't even have real data observations!

$$\text{Assumption:}\quad \frac{\color{blue} p_\text{data}(\mathbf{x})}{\color{red} p_\text{sim}(\mathbf{x})} \approx \frac{\color{blue} p_\text{data-control}(\mathbf{x})}{\color{red} p_\text{sim-control}(\mathbf{x})} = r(\mathbf{x}) $$

\vspace{1cm}

In the likelihood-free setting, estimating $r(\mathbf{x})$ is known as the {\it
density-ratio estimation} problem. Same as
\begin{itemize}
    \item Learning under covariate shift,
    \item Probabilistic classification,
    \item Likelihood-ratio test,
    \item Outlier detection,
    \item Mutual information estimation, ...
\end{itemize}
See \cite{sugiyama2012density} for a review.
\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}{Conclusions}

\begin{itemize}
    \item Formulating appropriate machine learning tasks is {\color{red} difficult}.

    \vspace{0.5cm}

    \item On purpose or {\color{red} unwillingly}, simulation versus real data artefacts
          could be exploited to maximize classifiers accuracy.

    \vspace{0.5cm}

    \item Physically more correct classifiers can be obtained e.g. with {\color{blue} density-ratio reweighting}.
\end{itemize}

\end{frame}

% ------------------------------------------------------------------------------

\begin{frame}[plain,noframenumbering]
    \frametitle{References}
    {\footnotesize
    \bibliographystyle{apalike}
    \bibliography{biblio}}
\end{frame}

\end{document}
